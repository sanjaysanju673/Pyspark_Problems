{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''<br>\n",
    "    @Author: v sanjay kumar<br>\n",
    "    @Date: 2024-09-06 04:00:30<br>\n",
    "    @Last Modified by: v sanjay kumar<br>\n",
    "    @Last Modified time: 2024-09-07 04:00:30.<br>\n",
    "    @Title :All covid problems in pyspark <br>\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import max, round\n",
    "from pyspark.sql.functions import coalesce, col\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.find death percentage  globaly  and localy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------global percentage ---------\n",
      "+--------------+----------------+\n",
      "|Country/Region|Death_percentage|\n",
      "+--------------+----------------+\n",
      "|          Chad|          8.5993|\n",
      "|      Paraguay|          1.0635|\n",
      "|        Russia|          1.3640|\n",
      "|         Yemen|         26.3575|\n",
      "|       Senegal|          1.5353|\n",
      "|    Cabo Verde|          1.0322|\n",
      "|        Sweden|          9.0267|\n",
      "|        Guyana|          7.0512|\n",
      "|       Eritrea|          0.0000|\n",
      "|   Philippines|          3.7305|\n",
      "|         Burma|          2.5369|\n",
      "|      Djibouti|          0.8956|\n",
      "|      Malaysia|          1.4792|\n",
      "|     Singapore|          0.0697|\n",
      "|          Fiji|          0.0000|\n",
      "|        Turkey|          2.6032|\n",
      "|        Malawi|          1.8290|\n",
      "|Western Sahara|          6.9922|\n",
      "|          Iraq|          3.9239|\n",
      "|       Germany|          4.1375|\n",
      "+--------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "----------local_percentage-----------\n",
      "+--------------------+----------------+\n",
      "|State/UnionTerritory|Death_percentage|\n",
      "+--------------------+----------------+\n",
      "|Andaman and Nicob...|            1.40|\n",
      "|      Andhra Pradesh|            0.75|\n",
      "|   Arunachal Pradesh|            0.37|\n",
      "|               Assam|            0.64|\n",
      "|               Bihar|            0.83|\n",
      "|           Bihar****|            1.32|\n",
      "|Cases being reass...|            0.00|\n",
      "|          Chandigarh|            1.36|\n",
      "|        Chhattisgarh|            1.26|\n",
      "|Dadra and Nagar H...|            0.04|\n",
      "|Dadra and Nagar H...|            0.05|\n",
      "|         Daman & Diu|            0.00|\n",
      "|               Delhi|            1.72|\n",
      "|                 Goa|            1.59|\n",
      "|             Gujarat|            1.55|\n",
      "|             Haryana|            1.12|\n",
      "|    Himachal Pradesh|            1.64|\n",
      "|   Himanchal Pradesh|            1.71|\n",
      "|   Jammu and Kashmir|            1.44|\n",
      "|           Jharkhand|            1.21|\n",
      "+--------------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Find the global\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Corrected file path\n",
    "    file_path = r\"file:///C:\\Users\\sanju\\OneDrive\\Documents\\dataset\\covid_19_clean_complete.csv\"\n",
    "    df_global = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "    file_path = r\"file:///C:\\Users\\sanju\\OneDrive\\Documents\\dataset\\Covid Problem\\covid_19_india.csv\"\n",
    "    df_local = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "    df_local.createOrReplaceTempView(\"covid_19_india\")\n",
    "    df_global.createOrReplaceTempView(\"country\")\n",
    "    global_result = spark.sql('''\n",
    "    SELECT `Country/Region`, \n",
    "           ROUND((SUM(Deaths) * 100.0 / SUM(Confirmed)), 4) AS Death_percentage\n",
    "    FROM country\n",
    "    GROUP BY `Country/Region`\n",
    "''')\n",
    "   \n",
    "    local_result=spark.sql('''\n",
    "    SELECT `State/UnionTerritory`, \n",
    "           ROUND((SUM(CAST(Deaths AS INT)) * 100.0) / SUM(CAST(Confirmed AS INT)), 2) AS Death_percentage\n",
    "    FROM covid_19_india\n",
    "    GROUP BY `State/UnionTerritory`\n",
    "    ORDER BY `State/UnionTerritory`\n",
    "''')\n",
    "    print(\"---------global percentage ---------\")\n",
    "    global_result.show()\n",
    "    print(\"----------local_percentage-----------\")\n",
    "    local_result.show()\n",
    "    spark.stop()\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. To find out the infected population percentage locally and globally\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------globla infected percentage----------\n",
      "+--------------------+-----------+------------+----------------------------+\n",
      "|State/UnionTerritory|Total_Cases|Total_Active|Globally_Infected_percentage|\n",
      "+--------------------+-----------+------------+----------------------------+\n",
      "|            Nagaland|    5041742|        NULL|                        NULL|\n",
      "|           Karnataka|  485970693|       405.0|        8.333835884214524E-5|\n",
      "|              Odisha|  160130533|        23.0|        1.436328198570350...|\n",
      "|              Kerala|  458906023|      1091.0|        2.377393072480986E-4|\n",
      "|              Ladakh|    4054293|       162.0|         0.00399576448964098|\n",
      "|Dadra and Nagar H...|    1938632|        NULL|                        NULL|\n",
      "|          Tamil Nadu|  431928644|       138.0|        3.194972176932076...|\n",
      "|           Telengana|   69990668|       246.0|        3.514754281242179...|\n",
      "|        Chhattisgarh|  163776262|        25.0|        1.526472743650725...|\n",
      "|      Maharashtra***|    6229596|        NULL|                        NULL|\n",
      "|      Andhra Pradesh|  392432753|        81.0|        2.064047900711284...|\n",
      "|         Lakshadweep|     915784|        NULL|                        NULL|\n",
      "|      Madhya Pradesh|  135625265|       105.0|        7.741920356800777E-5|\n",
      "|              Punjab|   99949702|       231.0|        2.311162468498405...|\n",
      "|             Manipur|   12617943|         5.0|        3.962611021463641E-5|\n",
      "|         Daman & Diu|          2|        NULL|                        NULL|\n",
      "|Cases being reass...|     345565|        NULL|                        NULL|\n",
      "|                 Goa|   28240159|         9.0|        3.186950895000272E-5|\n",
      "|             Mizoram|    2984732|         4.0|        1.340153822855787...|\n",
      "|           Bihar****|    1430909|        NULL|                        NULL|\n",
      "+--------------------+-----------+------------+----------------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "-----------local infected percentage-----------\n",
      "+--------------------+---------------+-----------+-----------------+\n",
      "|State/UnionTerritory|Total_Confirmed|Total_Cured| locally_Infected|\n",
      "+--------------------+---------------+-----------+-----------------+\n",
      "|            Nagaland|        5041742|    4519526| 10.3578485372714|\n",
      "|           Karnataka|      485970693|  441844360|  9.0800399356593|\n",
      "|              Odisha|      160130533|  150923455|  5.7497329381899|\n",
      "|              Kerala|      458906023|  420174235|  8.4400260747940|\n",
      "|              Ladakh|        4054293|    3758960|  7.2844513211058|\n",
      "|Dadra and Nagar H...|        1938632|    1841750|  4.9974414948273|\n",
      "|          Tamil Nadu|      431928644|  404095807|  6.4438507116004|\n",
      "|           Telengana|       69990668|   64666267|  7.6073013047968|\n",
      "|        Chhattisgarh|      163776262|  151609364|  7.4289752687114|\n",
      "|      Maharashtra***|        6229596|    6000911|  3.6709443116375|\n",
      "|      Andhra Pradesh|      392432753|  370426530|  5.6076417760166|\n",
      "|         Lakshadweep|         915784|     820925| 10.3582285779179|\n",
      "|      Madhya Pradesh|      135625265|  126724997|  6.5623967628745|\n",
      "|              Punjab|       99949702|   91458159|  8.4958162256452|\n",
      "|             Manipur|       12617943|   11230568| 10.9952549318062|\n",
      "|         Daman & Diu|              2|          0|100.0000000000000|\n",
      "|Cases being reass...|         345565|          0|100.0000000000000|\n",
      "|                 Goa|       28240159|   26027201|  7.8362094207756|\n",
      "|             Mizoram|        2984732|    2384602| 20.1066628427611|\n",
      "|           Bihar****|        1430909|    1402468|  1.9876176612209|\n",
      "+--------------------+---------------+-----------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Start Spark session\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Find the infected population percentage\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Load local dataset (India specific)\n",
    "    file_path_local = r\"file:///C:/Users/sanju/OneDrive/Documents/dataset/Covid Problem/covid_19_india.csv\"\n",
    "    df = spark.read.csv(file_path_local, header=True, inferSchema=True)\n",
    "    df.createOrReplaceTempView(\"country\")\n",
    "    global_infected=spark.sql('''\n",
    "    SELECT \n",
    "    `State/UnionTerritory`, \n",
    "        SUM(`Confirmed`) AS Total_Cases, \n",
    "        SUM(`ConfirmedIndianNational`) AS Total_Active,\n",
    "        (SUM(`ConfirmedIndianNational`) * 100.0 / SUM(`Confirmed`)) AS Globally_Infected_percentage\n",
    "    FROM \n",
    "        country\n",
    "    GROUP BY \n",
    "        `State/UnionTerritory` ''')\n",
    "    localy_infected=spark.sql('''\n",
    "    SELECT \n",
    "    `State/UnionTerritory`, \n",
    "    SUM(`Confirmed`) AS Total_Confirmed, \n",
    "    SUM(`Cured`) AS Total_Cured,\n",
    "    CASE \n",
    "        WHEN SUM(`Confirmed`) > 0 THEN \n",
    "            100 - (SUM(`Cured`) * 100.0 / SUM(`Confirmed`))\n",
    "        ELSE \n",
    "            NULL \n",
    "    END AS locally_Infected\n",
    "    FROM \n",
    "        country\n",
    "    GROUP BY \n",
    "        `State/UnionTerritory`\n",
    "''')\n",
    "    print(\"-----------globla infected percentage----------\")\n",
    "    global_infected.show()\n",
    "    print(\"-----------local infected percentage-----------\")\n",
    "    localy_infected.show()\n",
    "    spark.stop()\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.The countries with the highest infection rates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Country/Region: string (nullable = true)\n",
      " |-- Confirmed: integer (nullable = true)\n",
      " |-- Deaths: integer (nullable = true)\n",
      " |-- Recovered: integer (nullable = true)\n",
      " |-- Active: integer (nullable = true)\n",
      " |-- New cases: integer (nullable = true)\n",
      " |-- New deaths: integer (nullable = true)\n",
      " |-- New recovered: integer (nullable = true)\n",
      " |-- Deaths / 100 Cases: double (nullable = true)\n",
      " |-- Recovered / 100 Cases: double (nullable = true)\n",
      " |-- Deaths / 100 Recovered: string (nullable = true)\n",
      " |-- Confirmed last week: integer (nullable = true)\n",
      " |-- 1 week change: integer (nullable = true)\n",
      " |-- 1 week % increase: double (nullable = true)\n",
      " |-- WHO Region: string (nullable = true)\n",
      "\n",
      "+--------------+-------------------------+\n",
      "|Country/Region|Infection_Rate_Percentage|\n",
      "+--------------+-------------------------+\n",
      "|            US|                    26.03|\n",
      "|        Brazil|                    14.82|\n",
      "|         India|                     8.98|\n",
      "|        Russia|                     4.96|\n",
      "|  South Africa|                     2.75|\n",
      "|        Mexico|                      2.4|\n",
      "|          Peru|                     2.36|\n",
      "|         Chile|                     2.11|\n",
      "|United Kingdom|                     1.83|\n",
      "|          Iran|                     1.78|\n",
      "+--------------+-------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Find the global and local death percentage\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Corrected file path\n",
    "    file_path = r\"file:///C:\\Users\\sanju\\OneDrive\\Documents\\dataset\\country_wise_latest.csv\"\n",
    "    \n",
    "\n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "    df.printSchema()\n",
    "    # df.show()\n",
    "    df.createOrReplaceTempView(\"country\")\n",
    "    result = spark.sql('''\n",
    "    SELECT `Country/Region`, \n",
    "           ROUND((CAST(`Confirmed` AS FLOAT) / (SELECT SUM(CAST(`Confirmed` AS FLOAT)) FROM country)) * 100, 2) AS Infection_Rate_Percentage\n",
    "    FROM country\n",
    "    GROUP BY `Country/Region`, `Confirmed`\n",
    "    ORDER BY Infection_Rate_Percentage DESC\n",
    "''')\n",
    "\n",
    "    result.show(10)\n",
    "    \n",
    "    spark.stop()\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.The countries and continents with the highest death counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Country/Region: string (nullable = true)\n",
      " |-- Confirmed: integer (nullable = true)\n",
      " |-- Deaths: integer (nullable = true)\n",
      " |-- Recovered: integer (nullable = true)\n",
      " |-- Active: integer (nullable = true)\n",
      " |-- New cases: integer (nullable = true)\n",
      " |-- New deaths: integer (nullable = true)\n",
      " |-- New recovered: integer (nullable = true)\n",
      " |-- Deaths / 100 Cases: double (nullable = true)\n",
      " |-- Recovered / 100 Cases: double (nullable = true)\n",
      " |-- Deaths / 100 Recovered: string (nullable = true)\n",
      " |-- Confirmed last week: integer (nullable = true)\n",
      " |-- 1 week change: integer (nullable = true)\n",
      " |-- 1 week % increase: double (nullable = true)\n",
      " |-- WHO Region: string (nullable = true)\n",
      "\n",
      "+--------------+--------------------+------+\n",
      "|Country/Region|          WHO Region|Deaths|\n",
      "+--------------+--------------------+------+\n",
      "|            US|            Americas|148011|\n",
      "|        Brazil|            Americas| 87618|\n",
      "|United Kingdom|              Europe| 45844|\n",
      "|        Mexico|            Americas| 44022|\n",
      "|         Italy|              Europe| 35112|\n",
      "|         India|     South-East Asia| 33408|\n",
      "|        France|              Europe| 30212|\n",
      "|         Spain|              Europe| 28432|\n",
      "|          Peru|            Americas| 18418|\n",
      "|          Iran|Eastern Mediterra...| 15912|\n",
      "+--------------+--------------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Find hight death counts\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Corrected file path\n",
    "    file_path = r\"file:///C:\\Users\\sanju\\OneDrive\\Documents\\dataset\\country_wise_latest.csv\"\n",
    "    \n",
    "\n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "    df.printSchema()\n",
    "    # df.show()\n",
    "    df.createOrReplaceTempView(\"country\")\n",
    "    result = spark.sql('''\n",
    "        \n",
    "    SELECT `Country/Region`,`WHO Region`, Deaths\n",
    "    FROM country\n",
    "    ORDER BY Deaths DESC;\n",
    "    \n",
    "''')\n",
    "\n",
    "    result.show(10)\n",
    "    \n",
    "    spark.stop()\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.Average number of deaths by day (Continents and Countries) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Avarage number of deaths\") \\\n",
    "        .getOrCreate()\n",
    "    # Corrected file path\n",
    "    file_path = r\"file:///C:\\Users\\sanju\\OneDrive\\Documents\\dataset\\worldometer_data.csv\"\n",
    "    \n",
    "\n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "    df.printSchema()\n",
    "    df.createOrReplaceTempView(\"worldometer_data\")\n",
    "    result = spark.sql('''\n",
    "    SELECT\n",
    "        `Country/Region`,\n",
    "        Population,\n",
    "        TotalCases,\n",
    "        CASE \n",
    "            WHEN Population = 0 THEN NULL\n",
    "            ELSE CAST(TotalCases AS FLOAT) / CAST(Population AS FLOAT)\n",
    "        END AS CasesPerPopulation\n",
    "    FROM\n",
    "        worldometer_data\n",
    "    WHERE\n",
    "        Population RLIKE '^[0-9]+$'\n",
    "        AND TotalCases RLIKE '^[0-9]+$'\n",
    "        AND CAST(Population AS FLOAT) <> 0\n",
    "    ORDER BY\n",
    "        CasesPerPopulation DESC ''')\n",
    "    result.show(10)\n",
    "\n",
    "    spark.stop()\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 6.Average of cases divided by the number of population of each country (TOP 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Country/Region: string (nullable = true)\n",
      " |-- Continent: string (nullable = true)\n",
      " |-- Population: integer (nullable = true)\n",
      " |-- TotalCases: integer (nullable = true)\n",
      " |-- NewCases: integer (nullable = true)\n",
      " |-- TotalDeaths: integer (nullable = true)\n",
      " |-- NewDeaths: integer (nullable = true)\n",
      " |-- TotalRecovered: integer (nullable = true)\n",
      " |-- NewRecovered: integer (nullable = true)\n",
      " |-- ActiveCases: integer (nullable = true)\n",
      " |-- Serious,Critical: integer (nullable = true)\n",
      " |-- Tot Cases/1M pop: integer (nullable = true)\n",
      " |-- Deaths/1M pop: double (nullable = true)\n",
      " |-- TotalTests: integer (nullable = true)\n",
      " |-- Tests/1M pop: integer (nullable = true)\n",
      " |-- WHO Region: string (nullable = true)\n",
      "\n",
      "+--------------+------------+----------+--------------------+\n",
      "|Country/Region|  Population|TotalCases|  CasesPerPopulation|\n",
      "+--------------+------------+----------+--------------------+\n",
      "|         Qatar|   2807805.0|  112092.0|0.039921575750452756|\n",
      "| French Guiana|    299385.0|    8127.0|0.027145648579588157|\n",
      "|       Bahrain|   1706669.0|   42889.0| 0.02513023907975126|\n",
      "|    San Marino|     33938.0|     699.0|0.020596381637102954|\n",
      "|         Chile| 1.9132514E7|  366671.0|0.019164810228284687|\n",
      "|        Panama|   4321282.0|   71418.0| 0.01652703989232825|\n",
      "|        Kuwait|   4276658.0|   70045.0|0.016378443167538764|\n",
      "|          Oman|   5118446.0|   80713.0|0.015769043963734304|\n",
      "|           USA|3.31198144E8| 5032179.0|0.015193862960518527|\n",
      "|  Vatican City|       801.0|      12.0|  0.0149812734082397|\n",
      "+--------------+------------+----------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Avarage of cases divided by the population\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Corrected file path\n",
    "    file_path = r\"file:///C:\\Users\\sanju\\OneDrive\\Documents\\dataset\\worldometer_data.csv\"\n",
    "    \n",
    "\n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "    df.printSchema()\n",
    "    df.createOrReplaceTempView(\"worldometer_data\")\n",
    "    result=spark.sql('''\n",
    "    SELECT `Country/Region`,\n",
    "           CAST(Population AS FLOAT) AS Population,\n",
    "           CAST(TotalCases AS FLOAT) AS TotalCases,\n",
    "           CASE \n",
    "               WHEN CAST(Population AS FLOAT) = 0 THEN NULL\n",
    "               ELSE CAST(TotalCases AS FLOAT) / CAST(Population AS FLOAT)\n",
    "           END AS CasesPerPopulation\n",
    "    FROM worldometer_data\n",
    "    WHERE Population RLIKE '^[0-9]+$'\n",
    "      AND TotalCases RLIKE '^[0-9]+$'\n",
    "      AND CAST(Population AS FLOAT) <> 0\n",
    "    ORDER BY CasesPerPopulation DESC\n",
    "''')\n",
    "    result.show(10)\n",
    "\n",
    "    spark.stop()\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
