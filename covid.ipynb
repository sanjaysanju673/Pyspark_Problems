{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''<br>\n",
    "    @Author: v sanjay kumar<br>\n",
    "    @Date: 2024-09-06 04:00:30<br>\n",
    "    @Last Modified by: v sanjay kumar<br>\n",
    "    @Last Modified time: 2024-09-07 04:00:30.<br>\n",
    "    @Title :All covid problems in pyspark <br>\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import max, round\n",
    "from pyspark.sql.functions import coalesce, col\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.find death percentage  globaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Province/State: string (nullable = true)\n",
      " |-- Country/Region: string (nullable = true)\n",
      " |-- Lat: double (nullable = true)\n",
      " |-- Long: double (nullable = true)\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Confirmed: integer (nullable = true)\n",
      " |-- Deaths: integer (nullable = true)\n",
      " |-- Recovered: integer (nullable = true)\n",
      " |-- Active: integer (nullable = true)\n",
      " |-- WHO Region: string (nullable = true)\n",
      "\n",
      "+--------------+----------------+\n",
      "|Country/Region|Death_percentage|\n",
      "+--------------+----------------+\n",
      "|          Chad|          8.5993|\n",
      "|      Paraguay|          1.0635|\n",
      "|        Russia|          1.3640|\n",
      "|         Yemen|         26.3575|\n",
      "|       Senegal|          1.5353|\n",
      "|    Cabo Verde|          1.0322|\n",
      "|        Sweden|          9.0267|\n",
      "|        Guyana|          7.0512|\n",
      "|       Eritrea|          0.0000|\n",
      "|   Philippines|          3.7305|\n",
      "|         Burma|          2.5369|\n",
      "|      Djibouti|          0.8956|\n",
      "|      Malaysia|          1.4792|\n",
      "|     Singapore|          0.0697|\n",
      "|          Fiji|          0.0000|\n",
      "|        Turkey|          2.6032|\n",
      "|        Malawi|          1.8290|\n",
      "|Western Sahara|          6.9922|\n",
      "|          Iraq|          3.9239|\n",
      "|       Germany|          4.1375|\n",
      "+--------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Find the global\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Corrected file path\n",
    "    file_path = r\"file:///C:\\Users\\sanju\\OneDrive\\Documents\\dataset\\covid_19_clean_complete.csv\"\n",
    "    df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "    df.printSchema()\n",
    "    df.createOrReplaceTempView(\"country\")\n",
    "    result = spark.sql('''\n",
    "    SELECT `Country/Region`, \n",
    "           ROUND((SUM(Deaths) * 100.0 / SUM(Confirmed)), 4) AS Death_percentage\n",
    "    FROM country\n",
    "    GROUP BY `Country/Region`\n",
    "''')\n",
    "    result.show()\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.find death percentage  localy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+\n",
      "|State/UnionTerritory|Death_percentage|\n",
      "+--------------------+----------------+\n",
      "|Andaman and Nicob...|            1.40|\n",
      "|      Andhra Pradesh|            0.75|\n",
      "|   Arunachal Pradesh|            0.37|\n",
      "|               Assam|            0.64|\n",
      "|               Bihar|            0.83|\n",
      "|           Bihar****|            1.32|\n",
      "|Cases being reass...|            0.00|\n",
      "|          Chandigarh|            1.36|\n",
      "|        Chhattisgarh|            1.26|\n",
      "|Dadra and Nagar H...|            0.04|\n",
      "|Dadra and Nagar H...|            0.05|\n",
      "|         Daman & Diu|            0.00|\n",
      "|               Delhi|            1.72|\n",
      "|                 Goa|            1.59|\n",
      "|             Gujarat|            1.55|\n",
      "|             Haryana|            1.12|\n",
      "|    Himachal Pradesh|            1.64|\n",
      "|   Himanchal Pradesh|            1.71|\n",
      "|   Jammu and Kashmir|            1.44|\n",
      "|           Jharkhand|            1.21|\n",
      "+--------------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- Sno: integer (nullable = true)\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      " |-- State/UnionTerritory: string (nullable = true)\n",
      " |-- ConfirmedIndianNational: string (nullable = true)\n",
      " |-- ConfirmedForeignNational: string (nullable = true)\n",
      " |-- Cured: integer (nullable = true)\n",
      " |-- Deaths: integer (nullable = true)\n",
      " |-- Confirmed: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Find the global\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Corrected file path\n",
    "    file_path = r\"file:///C:\\Users\\sanju\\OneDrive\\Documents\\dataset\\Covid Problem\\covid_19_india.csv\"\n",
    "    df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "    df.createOrReplaceTempView(\"covid_19_india\")\n",
    "    result=spark.sql('''\n",
    "    SELECT `State/UnionTerritory`, \n",
    "           ROUND((SUM(CAST(Deaths AS INT)) * 100.0) / SUM(CAST(Confirmed AS INT)), 2) AS Death_percentage\n",
    "    FROM covid_19_india\n",
    "    GROUP BY `State/UnionTerritory`\n",
    "    ORDER BY `State/UnionTerritory`\n",
    "''')\n",
    "    result.show()\n",
    "    spark.stop()\n",
    "    df.printSchema()\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.The countries with the highest infection rates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Country/Region: string (nullable = true)\n",
      " |-- Confirmed: integer (nullable = true)\n",
      " |-- Deaths: integer (nullable = true)\n",
      " |-- Recovered: integer (nullable = true)\n",
      " |-- Active: integer (nullable = true)\n",
      " |-- New cases: integer (nullable = true)\n",
      " |-- New deaths: integer (nullable = true)\n",
      " |-- New recovered: integer (nullable = true)\n",
      " |-- Deaths / 100 Cases: double (nullable = true)\n",
      " |-- Recovered / 100 Cases: double (nullable = true)\n",
      " |-- Deaths / 100 Recovered: string (nullable = true)\n",
      " |-- Confirmed last week: integer (nullable = true)\n",
      " |-- 1 week change: integer (nullable = true)\n",
      " |-- 1 week % increase: double (nullable = true)\n",
      " |-- WHO Region: string (nullable = true)\n",
      "\n",
      "+--------------+-------------------------+\n",
      "|Country/Region|Infection_Rate_Percentage|\n",
      "+--------------+-------------------------+\n",
      "|            US|                    26.03|\n",
      "|        Brazil|                    14.82|\n",
      "|         India|                     8.98|\n",
      "|        Russia|                     4.96|\n",
      "|  South Africa|                     2.75|\n",
      "|        Mexico|                      2.4|\n",
      "|          Peru|                     2.36|\n",
      "|         Chile|                     2.11|\n",
      "|United Kingdom|                     1.83|\n",
      "|          Iran|                     1.78|\n",
      "+--------------+-------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Find the global and local death percentage\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Corrected file path\n",
    "    file_path = r\"file:///C:\\Users\\sanju\\OneDrive\\Documents\\dataset\\country_wise_latest.csv\"\n",
    "    \n",
    "\n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "    df.printSchema()\n",
    "    # df.show()\n",
    "    df.createOrReplaceTempView(\"country\")\n",
    "    result = spark.sql('''\n",
    "    SELECT `Country/Region`, \n",
    "           ROUND((CAST(`Confirmed` AS FLOAT) / (SELECT SUM(CAST(`Confirmed` AS FLOAT)) FROM country)) * 100, 2) AS Infection_Rate_Percentage\n",
    "    FROM country\n",
    "    GROUP BY `Country/Region`, `Confirmed`\n",
    "    ORDER BY Infection_Rate_Percentage DESC\n",
    "''')\n",
    "\n",
    "    result.show(10)\n",
    "    \n",
    "    spark.stop()\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.The countries and continents with the highest death counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Country/Region: string (nullable = true)\n",
      " |-- Confirmed: integer (nullable = true)\n",
      " |-- Deaths: integer (nullable = true)\n",
      " |-- Recovered: integer (nullable = true)\n",
      " |-- Active: integer (nullable = true)\n",
      " |-- New cases: integer (nullable = true)\n",
      " |-- New deaths: integer (nullable = true)\n",
      " |-- New recovered: integer (nullable = true)\n",
      " |-- Deaths / 100 Cases: double (nullable = true)\n",
      " |-- Recovered / 100 Cases: double (nullable = true)\n",
      " |-- Deaths / 100 Recovered: string (nullable = true)\n",
      " |-- Confirmed last week: integer (nullable = true)\n",
      " |-- 1 week change: integer (nullable = true)\n",
      " |-- 1 week % increase: double (nullable = true)\n",
      " |-- WHO Region: string (nullable = true)\n",
      "\n",
      "+--------------+--------------------+------+\n",
      "|Country/Region|          WHO Region|Deaths|\n",
      "+--------------+--------------------+------+\n",
      "|            US|            Americas|148011|\n",
      "|        Brazil|            Americas| 87618|\n",
      "|United Kingdom|              Europe| 45844|\n",
      "|        Mexico|            Americas| 44022|\n",
      "|         Italy|              Europe| 35112|\n",
      "|         India|     South-East Asia| 33408|\n",
      "|        France|              Europe| 30212|\n",
      "|         Spain|              Europe| 28432|\n",
      "|          Peru|            Americas| 18418|\n",
      "|          Iran|Eastern Mediterra...| 15912|\n",
      "+--------------+--------------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Find hight death counts\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Corrected file path\n",
    "    file_path = r\"file:///C:\\Users\\sanju\\OneDrive\\Documents\\dataset\\country_wise_latest.csv\"\n",
    "    \n",
    "\n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "    df.printSchema()\n",
    "    # df.show()\n",
    "    df.createOrReplaceTempView(\"country\")\n",
    "    result = spark.sql('''\n",
    "        \n",
    "    SELECT `Country/Region`,`WHO Region`, Deaths\n",
    "    FROM country\n",
    "    ORDER BY Deaths DESC;\n",
    "    \n",
    "''')\n",
    "\n",
    "    result.show(10)\n",
    "    \n",
    "    spark.stop()\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.Average number of deaths by day (Continents and Countries) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Country/Region: string (nullable = true)\n",
      " |-- Continent: string (nullable = true)\n",
      " |-- Population: integer (nullable = true)\n",
      " |-- TotalCases: integer (nullable = true)\n",
      " |-- NewCases: integer (nullable = true)\n",
      " |-- TotalDeaths: integer (nullable = true)\n",
      " |-- NewDeaths: integer (nullable = true)\n",
      " |-- TotalRecovered: integer (nullable = true)\n",
      " |-- NewRecovered: integer (nullable = true)\n",
      " |-- ActiveCases: integer (nullable = true)\n",
      " |-- Serious,Critical: integer (nullable = true)\n",
      " |-- Tot Cases/1M pop: integer (nullable = true)\n",
      " |-- Deaths/1M pop: double (nullable = true)\n",
      " |-- TotalTests: integer (nullable = true)\n",
      " |-- Tests/1M pop: integer (nullable = true)\n",
      " |-- WHO Region: string (nullable = true)\n",
      "\n",
      "+--------------+-------------+----------+----------+--------+-----------+---------+--------------+------------+-----------+----------------+----------------+-------------+----------+------------+--------------------+\n",
      "|Country/Region|    Continent|Population|TotalCases|NewCases|TotalDeaths|NewDeaths|TotalRecovered|NewRecovered|ActiveCases|Serious,Critical|Tot Cases/1M pop|Deaths/1M pop|TotalTests|Tests/1M pop|          WHO Region|\n",
      "+--------------+-------------+----------+----------+--------+-----------+---------+--------------+------------+-----------+----------------+----------------+-------------+----------+------------+--------------------+\n",
      "|           USA|North America| 331198130|   5032179|    NULL|     162804|     NULL|       2576668|        NULL|    2292707|           18296|           15194|        492.0|  63139605|      190640|            Americas|\n",
      "|        Brazil|South America| 212710692|   2917562|    NULL|      98644|     NULL|       2047660|        NULL|     771258|            8318|           13716|        464.0|  13206188|       62085|            Americas|\n",
      "|         India|         Asia|1381344997|   2025409|    NULL|      41638|     NULL|       1377384|        NULL|     606387|            8944|            1466|         30.0|  22149351|       16035|      South-EastAsia|\n",
      "|        Russia|       Europe| 145940924|    871894|    NULL|      14606|     NULL|        676357|        NULL|     180931|            2300|            5974|        100.0|  29716907|      203623|              Europe|\n",
      "|  South Africa|       Africa|  59381566|    538184|    NULL|       9604|     NULL|        387316|        NULL|     141264|             539|            9063|        162.0|   3149807|       53044|              Africa|\n",
      "|        Mexico|North America| 129066160|    462690|    6590|      50517|      819|        308848|        4140|     103325|            3987|            3585|        391.0|   1056915|        8189|            Americas|\n",
      "|          Peru|South America|  33016319|    455409|    NULL|      20424|     NULL|        310337|        NULL|     124648|            1426|           13793|        619.0|   2493429|       75521|            Americas|\n",
      "|         Chile|South America|  19132514|    366671|    NULL|       9889|     NULL|        340168|        NULL|      16614|            1358|           19165|        517.0|   1760615|       92022|            Americas|\n",
      "|      Colombia|South America|  50936262|    357710|    NULL|      11939|     NULL|        192355|        NULL|     153416|            1493|            7023|        234.0|   1801835|       35374|            Americas|\n",
      "|         Spain|       Europe|  46756648|    354530|    NULL|      28500|     NULL|          NULL|        NULL|       NULL|             617|            7582|        610.0|   7064329|      151087|              Europe|\n",
      "|          Iran|         Asia|  84097623|    320117|    NULL|      17976|     NULL|        277463|        NULL|      24678|            4156|            3806|        214.0|   2612763|       31068|EasternMediterranean|\n",
      "|            UK|       Europe|  67922029|    308134|    NULL|      46413|     NULL|          NULL|        NULL|       NULL|              73|            4537|        683.0|  17515234|      257873|              Europe|\n",
      "|  Saudi Arabia|         Asia|  34865919|    284226|    NULL|       3055|     NULL|        247089|        NULL|      34082|            1915|            8152|         88.0|   3635705|      104277|EasternMediterranean|\n",
      "|      Pakistan|         Asia| 221295851|    281863|    NULL|       6035|     NULL|        256058|        NULL|      19770|             809|            1274|         27.0|   2058872|        9304|EasternMediterranean|\n",
      "|    Bangladesh|         Asia| 164851401|    249651|    NULL|       3306|     NULL|        143824|        NULL|     102521|            NULL|            1514|         20.0|   1225124|        7432|      South-EastAsia|\n",
      "|         Italy|       Europe|  60452568|    249204|    NULL|      35187|     NULL|        201323|        NULL|      12694|              42|            4122|        582.0|   7099713|      117443|              Europe|\n",
      "|        Turkey|         Asia|  84428331|    237265|    NULL|       5798|     NULL|        220546|        NULL|      10921|             580|            2810|         69.0|   5081802|       60191|              Europe|\n",
      "|     Argentina|South America|  45236884|    228195|    NULL|       4251|     NULL|         99852|        NULL|     124092|            1150|            5044|         94.0|    794544|       17564|            Americas|\n",
      "|       Germany|       Europe|  83811260|    215210|    NULL|       9252|     NULL|        196200|        NULL|       9758|             236|            2568|        110.0|   8586648|      102452|              Europe|\n",
      "|        France|       Europe|  65288306|    195633|    NULL|      30312|     NULL|         82460|        NULL|      82861|             384|            2996|        464.0|   3992206|       61147|              Europe|\n",
      "+--------------+-------------+----------+----------+--------+-----------+---------+--------------+------------+-----------+----------------+----------------+-------------+----------+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "\n",
      "[PARSE_SYNTAX_ERROR] Syntax error at or near '#'.(line 2, pos 4)\n",
      "\n",
      "== SQL ==\n",
      "\n",
      "    # SELECT\n",
      "----^^^\n",
      "    #     `Country/Region`,\n",
      "    #     Population,\n",
      "    #     TotalCases,\n",
      "    #     CASE \n",
      "    #         WHEN Population = 0 THEN NULL\n",
      "    #         ELSE CAST(TotalCases AS FLOAT) / CAST(Population AS FLOAT)\n",
      "    #     END AS CasesPerPopulation\n",
      "    # FROM\n",
      "    #     worldometer_data\n",
      "    # WHERE\n",
      "    #     Population RLIKE '^[0-9]+$'\n",
      "    #     AND TotalCases RLIKE '^[0-9]+$'\n",
      "    #     AND CAST(Population AS FLOAT) <> 0\n",
      "    # ORDER BY\n",
      "    #     CasesPerPopulation DESC \n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Avarage number of deaths\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Corrected file path\n",
    "    file_path = r\"file:///C:\\Users\\sanju\\OneDrive\\Documents\\dataset\\worldometer_data.csv\"\n",
    "    \n",
    "\n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "    df.printSchema()\n",
    "    df.createOrReplaceTempView(\"worldometer_data\")\n",
    "    result = spark.sql('''\n",
    "    SELECT\n",
    "        `Country/Region`,\n",
    "        Population,\n",
    "        TotalCases,\n",
    "        CASE \n",
    "            WHEN Population = 0 THEN NULL\n",
    "            ELSE CAST(TotalCases AS FLOAT) / CAST(Population AS FLOAT)\n",
    "        END AS CasesPerPopulation\n",
    "    FROM\n",
    "        worldometer_data\n",
    "    WHERE\n",
    "        Population RLIKE '^[0-9]+$'\n",
    "        AND TotalCases RLIKE '^[0-9]+$'\n",
    "        AND CAST(Population AS FLOAT) <> 0\n",
    "    ORDER BY\n",
    "        CasesPerPopulation DESC ''')\n",
    "    result.show(10)\n",
    "\n",
    "    spark.stop()\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 6.Average of cases divided by the number of population of each country (TOP 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Country/Region: string (nullable = true)\n",
      " |-- Continent: string (nullable = true)\n",
      " |-- Population: integer (nullable = true)\n",
      " |-- TotalCases: integer (nullable = true)\n",
      " |-- NewCases: integer (nullable = true)\n",
      " |-- TotalDeaths: integer (nullable = true)\n",
      " |-- NewDeaths: integer (nullable = true)\n",
      " |-- TotalRecovered: integer (nullable = true)\n",
      " |-- NewRecovered: integer (nullable = true)\n",
      " |-- ActiveCases: integer (nullable = true)\n",
      " |-- Serious,Critical: integer (nullable = true)\n",
      " |-- Tot Cases/1M pop: integer (nullable = true)\n",
      " |-- Deaths/1M pop: double (nullable = true)\n",
      " |-- TotalTests: integer (nullable = true)\n",
      " |-- Tests/1M pop: integer (nullable = true)\n",
      " |-- WHO Region: string (nullable = true)\n",
      "\n",
      "+--------------+------------+----------+--------------------+\n",
      "|Country/Region|  Population|TotalCases|  CasesPerPopulation|\n",
      "+--------------+------------+----------+--------------------+\n",
      "|         Qatar|   2807805.0|  112092.0|0.039921575750452756|\n",
      "| French Guiana|    299385.0|    8127.0|0.027145648579588157|\n",
      "|       Bahrain|   1706669.0|   42889.0| 0.02513023907975126|\n",
      "|    San Marino|     33938.0|     699.0|0.020596381637102954|\n",
      "|         Chile| 1.9132514E7|  366671.0|0.019164810228284687|\n",
      "|        Panama|   4321282.0|   71418.0| 0.01652703989232825|\n",
      "|        Kuwait|   4276658.0|   70045.0|0.016378443167538764|\n",
      "|          Oman|   5118446.0|   80713.0|0.015769043963734304|\n",
      "|           USA|3.31198144E8| 5032179.0|0.015193862960518527|\n",
      "|  Vatican City|       801.0|      12.0|  0.0149812734082397|\n",
      "+--------------+------------+----------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Avarage of cases divided by the population\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Corrected file path\n",
    "    file_path = r\"file:///C:\\Users\\sanju\\OneDrive\\Documents\\dataset\\worldometer_data.csv\"\n",
    "    \n",
    "\n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "    df.printSchema()\n",
    "    df.createOrReplaceTempView(\"worldometer_data\")\n",
    "    result=spark.sql('''\n",
    "    SELECT `Country/Region`,\n",
    "           CAST(Population AS FLOAT) AS Population,\n",
    "           CAST(TotalCases AS FLOAT) AS TotalCases,\n",
    "           CASE \n",
    "               WHEN CAST(Population AS FLOAT) = 0 THEN NULL\n",
    "               ELSE CAST(TotalCases AS FLOAT) / CAST(Population AS FLOAT)\n",
    "           END AS CasesPerPopulation\n",
    "    FROM worldometer_data\n",
    "    WHERE Population RLIKE '^[0-9]+$'\n",
    "      AND TotalCases RLIKE '^[0-9]+$'\n",
    "      AND CAST(Population AS FLOAT) <> 0\n",
    "    ORDER BY CasesPerPopulation DESC\n",
    "''')\n",
    "    result.show(10)\n",
    "\n",
    "    spark.stop()\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Considering the highest value of total cases, which countries have the highest rate of infection in relation to population?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Country/Region: string (nullable = true)\n",
      " |-- Continent: string (nullable = true)\n",
      " |-- Population: integer (nullable = true)\n",
      " |-- TotalCases: integer (nullable = true)\n",
      " |-- NewCases: integer (nullable = true)\n",
      " |-- TotalDeaths: integer (nullable = true)\n",
      " |-- NewDeaths: integer (nullable = true)\n",
      " |-- TotalRecovered: integer (nullable = true)\n",
      " |-- NewRecovered: integer (nullable = true)\n",
      " |-- ActiveCases: integer (nullable = true)\n",
      " |-- Serious,Critical: integer (nullable = true)\n",
      " |-- Tot Cases/1M pop: integer (nullable = true)\n",
      " |-- Deaths/1M pop: double (nullable = true)\n",
      " |-- TotalTests: integer (nullable = true)\n",
      " |-- Tests/1M pop: integer (nullable = true)\n",
      " |-- WHO Region: string (nullable = true)\n",
      "\n",
      "+--------------+------------+----------+------------------+\n",
      "|Country/Region|  Population|TotalCases|   infliation_rate|\n",
      "+--------------+------------+----------+------------------+\n",
      "|         Qatar|   2807805.0|  112092.0| 399.2157802981332|\n",
      "| French Guiana|    299385.0|    8127.0| 271.4564857958816|\n",
      "|       Bahrain|   1706669.0|   42889.0| 251.3023814225254|\n",
      "|    San Marino|     33938.0|     699.0|205.96381637102954|\n",
      "|         Chile| 1.9132514E7|  366671.0|191.64810311911964|\n",
      "|        Panama|   4321282.0|   71418.0|165.27039151807264|\n",
      "|        Kuwait|   4276658.0|   70045.0|163.78442793414857|\n",
      "|          Oman|   5118446.0|   80713.0|157.69043651139427|\n",
      "|           USA|3.31198144E8| 5032179.0|151.93862636845202|\n",
      "|  Vatican City|       801.0|      12.0|149.81273408239701|\n",
      "|          Peru|  3.301632E7|  455409.0|137.93451607976044|\n",
      "|        Brazil|2.12710688E8| 2917562.0|137.16103929557053|\n",
      "|       Armenia|   2963811.0|   39819.0| 134.3506775566998|\n",
      "|       Andorra|     77278.0|     944.0|122.15637050648309|\n",
      "|    Luxembourg|    626952.0|    7073.0|112.81565414896197|\n",
      "|       Mayotte|    273419.0|    3042.0| 111.2578131000406|\n",
      "|     Singapore|   5854932.0|   54555.0| 93.17785689056679|\n",
      "|  South Africa| 5.9381568E7|  538184.0|  90.6314911263876|\n",
      "|        Israel|   9197590.0|   79559.0| 86.49983484804171|\n",
      "|      Maldives|    541448.0|    4680.0| 86.43489310146127|\n",
      "+--------------+------------+----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Avarage of cases divided by the population\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Corrected file path\n",
    "    file_path = r\"file:///C:\\Users\\sanju\\OneDrive\\Documents\\dataset\\worldometer_data.csv\"\n",
    "    \n",
    "\n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "    df.printSchema()\n",
    "    df.createOrReplaceTempView(\"worldometer_data\")\n",
    "    output=spark.sql('''\n",
    "    SELECT `Country/Region`,\n",
    "           CAST(Population AS FLOAT) AS Population,\n",
    "           CAST(TotalCases AS FLOAT) AS TotalCases,\n",
    "           CASE \n",
    "               WHEN CAST(Population AS FLOAT) = 0 THEN NULL\n",
    "               ELSE (CAST(TotalCases AS FLOAT) * 10000) / CAST(Population AS FLOAT)\n",
    "           END AS infliation_rate\n",
    "    FROM worldometer_data\n",
    "    WHERE Population RLIKE '^[0-9]+$'\n",
    "      AND TotalCases RLIKE '^[0-9]+$'\n",
    "      AND CAST(Population AS FLOAT) <> 0\n",
    "    ORDER BY infliation_rate DESC\n",
    "''')\n",
    "    output.show()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.To find out the population vs the number of people vaccinated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|vaccine_percentage|\n",
      "+------------------+\n",
      "|             33.53|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    spark = SparkSession.builder.appName(\"VaccinePercentage\").getOrCreate()\n",
    "\n",
    "    # Load data\n",
    "    worldometer_data = spark.read.csv(r\"file:///C:\\Users\\sanju\\OneDrive\\Documents\\dataset\\worldometer_data.csv\", header=True, inferSchema=True)\n",
    "    covid_vaccine_statewisenew = spark.read.csv(r\"file:///C:\\Users\\sanju\\OneDrive\\Documents\\dataset\\Covid Problem\\covid_vaccine_statewise.csv\", header=True, inferSchema=True)\n",
    "    # Get MaxPopulation for Asia\n",
    "    max_population = worldometer_data.filter(worldometer_data.Continent == 'Asia').agg(max(\"Population\").alias(\"MaxPopulation\"))\n",
    "\n",
    "    # Get the latest date for Total Doses Administered for India\n",
    "    latest_date = covid_vaccine_statewisenew.filter(\n",
    "        (covid_vaccine_statewisenew.State == 'India') &\n",
    "        (covid_vaccine_statewisenew[\"Total Doses Administered\"].isNotNull())\n",
    "    ).agg(\n",
    "        max(\"Updated On\").alias(\"LatestDate\")\n",
    "    ).collect()[0][\"LatestDate\"]\n",
    "\n",
    "    # Get Total Doses Administered for India on the latest date\n",
    "    latest_doses = covid_vaccine_statewisenew.filter(\n",
    "        (covid_vaccine_statewisenew.State == 'India') &\n",
    "        (covid_vaccine_statewisenew[\"Updated On\"] == latest_date)\n",
    "    ).select(\"Total Doses Administered\")\n",
    "\n",
    "    # Calculate vaccine percentage\n",
    "    vaccine_percentage = latest_doses.crossJoin(max_population).select(\n",
    "        round((latest_doses[\"Total Doses Administered\"] * 100.0) / max_population.MaxPopulation, 2).alias(\"vaccine_percentage\")\n",
    "    )\n",
    "\n",
    "    vaccine_percentage.show()\n",
    "    spark.stop()\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. To find out the percentage of different vaccine taken by people in a country\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-------------------+---------------------+--------------------+\n",
      "|      Date|               State| Covaxin_Percentage|CoviShield_Percentage|Sputnik_V_Percentage|\n",
      "+----------+--------------------+-------------------+---------------------+--------------------+\n",
      "|01/02/2021|Dadra and Nagar H...|                0.0|                100.0|                NULL|\n",
      "|01/02/2021|              Ladakh|                0.0|                100.0|                NULL|\n",
      "|01/02/2021|Andaman and Nicob...|                0.0|                100.0|                NULL|\n",
      "|01/02/2021|      Andhra Pradesh|  5.930021999072148|    94.06997800092785|                NULL|\n",
      "|01/02/2021|   Arunachal Pradesh|                0.0|                100.0|                NULL|\n",
      "|01/02/2021|               India|  2.070642173081831|    97.92935782691816|                NULL|\n",
      "|01/02/2021|               Bihar| 2.3935931512996653|    97.60640684870033|                NULL|\n",
      "|01/02/2021|          Chandigarh|                0.0|                100.0|                NULL|\n",
      "|01/02/2021|        Chhattisgarh|                0.0|                100.0|                NULL|\n",
      "|01/02/2021|               Assam|  7.398929753472115|    92.60107024652788|                NULL|\n",
      "|01/02/2021|               Delhi|  5.003790750568613|    94.99620924943139|                NULL|\n",
      "|01/02/2021|                 Goa|                0.0|                100.0|                NULL|\n",
      "|01/02/2021|             Gujarat| 6.4222120561635645|    93.57778794383644|                NULL|\n",
      "|01/02/2021|             Haryana| 3.7038959960126268|    96.29610400398737|                NULL|\n",
      "|01/02/2021|    Himachal Pradesh|                0.0|                100.0|                NULL|\n",
      "|01/02/2021|   Jammu and Kashmir|                0.0|                100.0|                NULL|\n",
      "|01/02/2021|           Jharkhand|0.05108948322987713|    99.94891051677013|                NULL|\n",
      "|01/02/2021|           Karnataka|  1.295157463459915|    98.70484253654008|                NULL|\n",
      "|01/02/2021|              Kerala|                0.0|                100.0|                NULL|\n",
      "|01/02/2021|         Lakshadweep|                0.0|                100.0|                NULL|\n",
      "+----------+--------------------+-------------------+---------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"VaccinePercentage\").getOrCreate()\n",
    "\n",
    "try:\n",
    "    covid_vaccine_statewisenew = spark.read.csv(r\"file:///C:\\Users\\sanju\\OneDrive\\Documents\\dataset\\Covid Problem\\covid_vaccine_statewise.csv\", header=True, inferSchema=True)\n",
    "\n",
    "    # Create a temporary view\n",
    "    covid_vaccine_statewisenew.createOrReplaceTempView(\"covid_vaccine_statewisenew\")\n",
    "    # Run the SQL query\n",
    "    result = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        `Updated On` as Date,\n",
    "        State,\n",
    "        ` Covaxin (Doses Administered)` * 100.0 / `Total Doses Administered` as Covaxin_Percentage,\n",
    "        `CoviShield (Doses Administered)` * 100.0 / `Total Doses Administered` as CoviShield_Percentage,\n",
    "        `Sputnik V (Doses Administered)` * 100.0 / `Total Doses Administered` as Sputnik_V_Percentage\n",
    "    FROM \n",
    "        covid_vaccine_statewisenew\n",
    "    ORDER BY Date ASC\n",
    "\"\"\")\n",
    "\n",
    "# Show the result\n",
    "    result.show()\n",
    "    spark.stop()\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. To find out percentage of people who took both the doses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+---------------------+\n",
      "|      Date|               State|Percentage_Both_Doses|\n",
      "+----------+--------------------+---------------------+\n",
      "|01/02/2021|Dadra and Nagar H...|                  0.0|\n",
      "|01/02/2021|              Ladakh|                  0.0|\n",
      "|01/02/2021|Andaman and Nicob...|                  0.0|\n",
      "|01/02/2021|      Andhra Pradesh|                  0.0|\n",
      "|01/02/2021|   Arunachal Pradesh|                  0.0|\n",
      "|01/02/2021|               India|                  0.0|\n",
      "|01/02/2021|               Bihar|                  0.0|\n",
      "|01/02/2021|          Chandigarh|                  0.0|\n",
      "|01/02/2021|        Chhattisgarh|                  0.0|\n",
      "|01/02/2021|               Assam|                  0.0|\n",
      "|01/02/2021|               Delhi|                  0.0|\n",
      "|01/02/2021|                 Goa|                  0.0|\n",
      "|01/02/2021|             Gujarat|                  0.0|\n",
      "|01/02/2021|             Haryana|                  0.0|\n",
      "|01/02/2021|    Himachal Pradesh|                  0.0|\n",
      "|01/02/2021|   Jammu and Kashmir|                  0.0|\n",
      "|01/02/2021|           Jharkhand|                  0.0|\n",
      "|01/02/2021|           Karnataka|                  0.0|\n",
      "|01/02/2021|              Kerala|                  0.0|\n",
      "|01/02/2021|         Lakshadweep|                  0.0|\n",
      "+----------+--------------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    spark = SparkSession.builder.appName(\"VaccinePercentage\").getOrCreate()\n",
    "\n",
    "    # Load data\n",
    "    covid_vaccine_statewisenew = spark.read.csv(r\"file:///C:\\Users\\sanju\\OneDrive\\Documents\\dataset\\Covid Problem\\covid_vaccine_statewise.csv\", header=True, inferSchema=True)\n",
    "\n",
    "    # Create a temporary view\n",
    "    covid_vaccine_statewisenew.createOrReplaceTempView(\"covid_vaccine_statewisenew\")\n",
    "\n",
    "    # Run the SQL query\n",
    "    result = spark.sql(\"\"\"\n",
    "        SELECT\n",
    "            covid_1.`Updated On` as Date,\n",
    "            covid_1.State,\n",
    "            COALESCE(covid_2.`Second Dose Administered`, 0) * 100.0 / COALESCE(covid_1.`First Dose Administered`, 1) as Percentage_Both_Doses\n",
    "        FROM \n",
    "            covid_vaccine_statewisenew covid_1\n",
    "        JOIN \n",
    "            covid_vaccine_statewisenew covid_2\n",
    "        ON \n",
    "            covid_1.State = covid_2.State\n",
    "            AND covid_1.`Updated On` = covid_2.`Updated On`\n",
    "        ORDER BY \n",
    "            covid_1.`Updated On` ASC\n",
    "    \"\"\")\n",
    "\n",
    "    # Show the result\n",
    "    result.show()\n",
    "    spark.stop()\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
